{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LARGE CNN\n",
    "\n",
    "### This model achieves 0.8 % error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional layer with 30 feature maps of size 5×5.\n",
    "- Pooling layer taking the max over 2*2 patches.\n",
    "- Convolutional layer with 15 feature maps of size 3×3.\n",
    "- Pooling layer taking the max over 2*2 patches.\n",
    "- Dropout layer with a probability of 20%.\n",
    "- Flatten layer.\n",
    "- Fully connected layer with 128 neurons and rectifier activation.\n",
    "- Fully connected layer with 50 neurons and rectifier activation.\n",
    "- Output layer.\n",
    "\n",
    "\n",
    "#### To do: try this model with the augmented dataset and ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the larger model\n",
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 142s 2ms/step - loss: 0.3901 - acc: 0.8791 - val_loss: 0.0803 - val_acc: 0.9731\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 142s 2ms/step - loss: 0.0942 - acc: 0.9703 - val_loss: 0.0495 - val_acc: 0.9836\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 137s 2ms/step - loss: 0.0694 - acc: 0.9786 - val_loss: 0.0359 - val_acc: 0.9880\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 137s 2ms/step - loss: 0.0583 - acc: 0.9819 - val_loss: 0.0332 - val_acc: 0.9892\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 138s 2ms/step - loss: 0.0477 - acc: 0.9847 - val_loss: 0.0282 - val_acc: 0.9909\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 138s 2ms/step - loss: 0.0417 - acc: 0.9869 - val_loss: 0.0300 - val_acc: 0.9894\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 136s 2ms/step - loss: 0.0381 - acc: 0.9876 - val_loss: 0.0290 - val_acc: 0.9906\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 136s 2ms/step - loss: 0.0373 - acc: 0.9884 - val_loss: 0.0286 - val_acc: 0.9899\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 137s 2ms/step - loss: 0.0334 - acc: 0.9892 - val_loss: 0.0283 - val_acc: 0.9898\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 136s 2ms/step - loss: 0.0293 - acc: 0.9905 - val_loss: 0.0243 - val_acc: 0.9920\n",
      "Large CNN Error: 0.80%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we have 0.8% error rate \n",
    "\n",
    "Overview on different ways to do ensemble learning https://en.wikipedia.org/wiki/Committee_machine\n",
    "\n",
    "Ensemble averaging (machine learning)\n",
    "In general ensemble learning works better if the individual predictors disagree a lot. So the main point is to make each NN very accurate, but with different wronk predictions.\n",
    "\n",
    "For ensemble learning:\n",
    "\n",
    "1) we make multiple NN based on different hyperparameters with the same architecture and then we build a last NN using the avarage of the weights of the NN we previously built. OF course in this case what we would like to have is that different NN correctly predict what the other NN predict wrongly. So toghether they should overlap and cover each other mistakes https://en.wikipedia.org/wiki/Ensemble_averaging_(machine_learning) \n",
    "\n",
    "    from Wikipedia \" Ensemble averaging keeps the less satisfactory networks around, but (gives them, in the averaging process,)     less weight.The theory of ensemble averaging relies on two properties of artificial neural networks:\n",
    "\n",
    "    1) In any network, the bias can be reduced at the cost of increased variance\n",
    "    2) In a group of networks, the variance can be reduced at no cost to bias\n",
    "\n",
    "    Ensemble averaging creates a group of networks, each with low bias and high variance, then combines them to a new network       with (hopefully) low bias and low variance. It is thus a resolution of the bias-variance dilemma. \" \n",
    "    I am not sure to have fully understood the variance/bias part, maybe one of you can help out on this?\n",
    "\n",
    "\n",
    "2) Or we can use different NN with different architecture and parameters and make a prediction based on the average of the probability output. So each NN output a probability, in this case an array of 10 probability( we are using one hot encoding) , we sum the the outputs and take the avarage. \n",
    "\n",
    "In both scenarios we can use augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save and load the weights is very simple in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##   We need h5py to save weights in this format\n",
    "\n",
    "## We save the weights : the file is usually saved in C:\\Users\\YourUserName\n",
    "\n",
    "import h5py \n",
    "##Saves the weights\n",
    "weigths=model.save_weights('my_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  To load the weights\n",
    "model.load_weights('my_model_weights.h5') \n",
    "\n",
    "##for info https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## just to transform one hot encoding back to classes\n",
    "## prediction[0].tolist().index(max(prediction[0]))==y_test[0].tolist().index(max(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
