# Non-Neural Network Solutions

## Random Forest

Found the script [here](https://www.kaggle.com/hideki1234/randomforest-of-tree-and-accuracy/code). 

Training a Random Forest using SciKit Learn for 150 trees per forest results in an accuracy of around .963, and adding more trees does not significantly improve performance. Given that the accuracy is lower than the initial machine learning method you guys tried out, which was .989, I believe this method is not worth pursuing.

## XGBoost

Found out about [XGBoost](http://xgboost.readthedocs.io/en/latest/) this morning in the Mercator kitchen. Short for "Extreme Gradient Boosting," it's used for supervised learning problems. I will take a shot at implementing a digit recognition solution sometime before Friday and figure out if it's better than the Random Forest model. 

